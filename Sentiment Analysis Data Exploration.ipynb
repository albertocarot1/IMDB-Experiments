{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "faaa97e2-d147-402b-8171-978744d47916",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/home/alberto/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cce3ef7cec6c4927a0e3f1d1addae6a0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: sst/default\n",
      "Found cached dataset sst (/home/alberto/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b2f63595d90740d088c0fcc2ba6f0a72"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datasets: Dict[str, Dataset] = {}\n",
    "\n",
    "datasets['ds_imdb'] = load_dataset(\"imdb\")\n",
    "datasets['sst'] = load_dataset(\"sst\")\n",
    "datasets['tweet'] = load_dataset(\"tweet_eval\", \"sentiment\")\n",
    "datasets['rotten'] = load_dataset(\"rotten_tomatoes\")\n",
    "datasets['amazon'] = load_dataset(\"amazon_reviews_multi\", \"en\")   # cannot use for money ever\n",
    "datasets['yelp'] = load_dataset(\"yelp_review_full\") # cannot use for money ever\n",
    "datasets['financ'] = load_dataset(\"financial_phrasebank\", \"sentences_75agree\")  # Must ask for commercial license\n",
    "datasets['amzpol'] = load_dataset(\"amazon_polarity\")\n",
    "datasets['movrat'] = load_dataset(\"movie_rationales\")\n",
    "datasets['multisen'] = load_dataset(\"tyqiangz/multilingual-sentiments\", \"english\")  # Check that it's different from the other amazon free one\n",
    "datasets['cyrpol'] = load_dataset(\"CyranoB/polarity\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "I want to see what the datasets are composed of: max, min, mean and median length of texts for each dataset (in terms of tokens)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "max_num_tokens = 512\n",
    "tokenizer_xlm = AutoTokenizer.from_pretrained(\"xlm-roberta-base\", model_max_length=max_num_tokens, truncation=True)\n",
    "tokenizer_xlnet = AutoTokenizer.from_pretrained(\"xlnet-large-cased\", model_max_length=max_num_tokens, truncation=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(datasets['ds_imdb'][\"train\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "df['tokenized_text_xlm'] = df['text'].apply(tokenizer_xlm)\n",
    "df['tokenized_text_xlnet'] = df['text'].apply(tokenizer_xlnet)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_num_tokens(tokens):\n",
    "    return len(tokens.encodings[0].ids)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['number_tokens_xlm'] = df['tokenized_text_xlm'].apply(get_num_tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.histplot(data=df, x=\"number_tokens_xlm\", bins=50, kde=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stats_token_counts = df.describe()['number_tokens_xlm']\n",
    "stats_token_counts.loc['median'] = df['number_tokens_xlm'].median()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stats_token_counts"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
