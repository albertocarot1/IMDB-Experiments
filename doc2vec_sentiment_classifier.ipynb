{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec sentiment classification (IMDB)\n",
    "Doc2Vec is a sentence embeddings representation, first introduced by Le & Mikolov in the paper \"Distributed Representations of Sentences and Documents\" <http://cs.stanford.edu/~quocle/paragraph_vector.pdf>.\n",
    "In this notebook we will replicate part of the experiments described in the paper, more specifically the sentiment classification on the IMDB dataset.\n",
    "\n",
    "The experiment basically consists in two main models trained:\n",
    "\n",
    "1. Doc2Vec model trained on train+unlabeled data, to obtain document embeddings on IMDB dataset.\n",
    "2. Logistic Regression trained on train data's Doc2Vec embeddings.\n",
    "\n",
    "In order to test accuracy of the system, the Doc2Vec model is used first to infer an embedding for each test document. These embeddings are then used to predict the sentiment of the review through the trained Logistic Regression.\n",
    "\n",
    "First of all we download the IMDB dataset from tensorflow_datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "\n",
    "data = tfds.load(\n",
    "    name=\"imdb_reviews\",\n",
    "    as_supervised=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create a class where the iterator is a generator that reads IMDB reviews, tokenizes them, and creates TaggedDocument to be fed into the Doc2Vec models. \n",
    "\n",
    "We need this class to stream data from disk during the Doc2Vec training and vocabulary creation. In this manner, the computer will not swap memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.utils import to_unicode\n",
    "\n",
    "def tokenize_text(text: str) -> list:\n",
    "    \"\"\" Clean from html tags and tokenize \"\"\"\n",
    "    # Standardize newlines\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    temp = re.sub(cleanr, '\\n', text)\n",
    "\t\n",
    "    # Remove repetition of blank spaces, by keeping only one\n",
    "    temp = re.sub(\" +\", \" \", temp)\n",
    "\n",
    "    clean = temp.strip()\n",
    "    return to_unicode(clean).split()\n",
    "\n",
    "\n",
    "class TxtCorpus(list):\n",
    "    \"\"\"\n",
    "    Iterable that returns TaggedDocument objects, used for Doc2Vec training.\n",
    "    Process documents one by one using generators, avoiding to fill up RAM.\n",
    "    \"\"\"\n",
    "    def __init__(self, tf_dataset, parts=['train', 'test', 'unsupervised']):\n",
    "        self.tf_dataset = tf_dataset\n",
    "        self.parts = parts\n",
    "\n",
    "    def __iter__(self):\n",
    "        for part in self.parts:\n",
    "            for i, line in enumerate(self.tf_dataset[part]):\n",
    "                text = line['text'].numpy().decode()\n",
    "                tokens = tokenize_text(text)\n",
    "                if i%10000 < 1:\n",
    "                    print(f\"DOCUMENT WITH ID(TAG): {part}-{i}\")\n",
    "                yield TaggedDocument(tokens, [f\"{part}-{i}\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embeddings for the training of the Logistic Regression model are calculated during the training of the Doc2Vec model. Instead, the embeddings for the test set, like in a real application scenario, must be inferred once the model is trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_from_model(d2v_model, tf_dataset, part):\n",
    "    \"\"\" Load embeddings created during training phase.\"\"\"\n",
    "    X = []\n",
    "    Y = []\n",
    "    for i, line in enumerate(tf_dataset[part]):\n",
    "        X.append(d2v_model.docvecs[f\"{part}-{i}\"])\n",
    "        Y.append(line['label'].numpy())\n",
    "    return np.asarray(X), np.asarray(Y)\n",
    "\n",
    "def infer_from_model(d2v_model, tf_dataset, part):\n",
    "    \"\"\" Infer embedding for a text not seen in training phase.\"\"\"\n",
    "    X = []\n",
    "    Y = []\n",
    "    for i, line in enumerate(tf_dataset[part]):\n",
    "        text = line['text'].numpy().decode()\n",
    "\n",
    "        X.append(d2v_model.infer_vector(tokenize_text(text)))\n",
    "        Y.append(line['label'].numpy())\n",
    "    return np.asarray(X), np.asarray(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training the logistic regression, a little bit of hyperparameter tuning is done in the process through grid search in a set of hyperparameters. The accuracy is then calculated on the best model found through a 5-fold cross validation. The limited number of features (typical of embeddings, compared to bag of words), allows us to do that relatively quickly. \n",
    "This way we expect to get that little extra boost in accuracy.\n",
    "\n",
    "Since the problem is binary classification, the dataset is balanced, and we do not have specific requirements on precision and recall, accuracy is an acceptable metric in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def train_lr_evaluate(model, data):\n",
    "    print(f\"Evaluating {model}\")\n",
    "    X_train, Y_train = load_from_model(model, data, 'train')\n",
    "    print(\"Training data embeddings loaded.\")\n",
    "\n",
    "    X_test, Y_test = infer_from_model(model, data, 'test')\n",
    "    print(\"Test data embeddings inferred.\")\n",
    "\n",
    "    lr = linear_model.LogisticRegression()\n",
    "\n",
    "    penalty = ['l1', 'l2']\n",
    "    C = [0.0001, 0.001, 0.1, 1, 10, 100]\n",
    "    hyperparameters = dict(C=C, penalty=penalty, solver=['liblinear'])\n",
    "\n",
    "    clf = GridSearchCV(lr, hyperparameters, cv=5, verbose=2, n_jobs=4)\n",
    "    print(f\"Training and testing logistic regression... \", end='')\n",
    "    start = time()\n",
    "    clf.fit(X_train, Y_train)\n",
    "    print(f\"finished in {time() - start} seconds\")\n",
    "    return clf.best_estimator_.score(X_test, Y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next steps, we combine unsupervised and train data for the Doc2Vec vocabulary creation and embeddings training. \n",
    "\n",
    "With some guidance from the gensim creators https://radimrehurek.com/gensim/auto_examples/howtos/run_doc2vec_imdb.html on parameters to optimize training time without affecting performances, we define the default parameters for the trained models:\n",
    "* ``vector_size=100``, as there not seems to be a decay in performances compared to the paper's size of 400.\n",
    "* ``epochs=20``, large enough for the model to learn features from the text.\n",
    "* ``min_count=2`` discards words that appear in only one document, and hence do not benefit from the training that relies on the co-occurence of words in different documents\n",
    "* ``sample=0``: no downsampling of frequent words\n",
    "* ``hs=0`` and ``negative=5``: 5 \"noise words\" are updated for every update of positive samples\n",
    " \n",
    "Starting from those base parameters, we train three models:\n",
    "\n",
    "* One with the Distributed Bag of word (``DBOW``) method described in the paper.\n",
    "* One with the Distiributed Memory (``DM``) method described in the paper, where the paragraph vector and word ``vectors are averaged during training`` to predict the next word in a context. \n",
    "* One with the Distiributed Memory (``DM``) method described in the paper, where the paragraph vector and word ``vectors are concatenated during training`` to predict the next word in a context. Concatenation results in a bigger and slower to train model, compared to averaging.\n",
    "\n",
    "In the paper, the authors noted that concatenating the trained DBOW and DM paragraph vectors improves performance. We will repeat the same experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "creating vocabularies...\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDoc2Vec(dbow,d100,n5,mc2,t8) vocabulary created\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDoc2Vec(\"alpha=0.05\",dm/m,d100,n5,w10,mc2,t8) vocabulary created\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDoc2Vec(dm/c,d100,n5,w5,mc2,t8) vocabulary created\nvocabularies created in 289.4932792186737 seconds\n"
    }
   ],
   "source": [
    "from time import time\n",
    "import multiprocessing\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "\n",
    "\n",
    "td_reviews = TxtCorpus(data, parts=['train', 'unsupervised'])\n",
    "\n",
    "common_params = dict(vector_size=100, epochs=20, min_count=2, sample=0, workers=multiprocessing.cpu_count(), negative=5, hs=0)\n",
    "\n",
    "models = [\n",
    "    # PV-DBOW training\n",
    "    Doc2Vec(dm=0, **common_params),\n",
    "    # PV-DM w/ default averaging; a higher starting learning rate may improve CBOW/PV-DM modes\n",
    "    Doc2Vec(dm=1, window=10, alpha=0.05, comment='alpha=0.05', **common_params),\n",
    "    # PV-DM w/ concatenation\n",
    "    Doc2Vec(dm=1, dm_concat=1, window=5, **common_params),\n",
    "]\n",
    "models_by_name = {}\n",
    "\n",
    "print(\"creating vocabularies...\")\n",
    "start = time()\n",
    "for model in models:\n",
    "    model.build_vocab(td_reviews)\n",
    "    print(f\"{model} vocabulary created\")\n",
    "    models_by_name[str(model)] = model\n",
    "print(f\"vocabularies created in {time() - start} seconds\")\n",
    "\n",
    "models_by_name['dbow+dmm'] = ConcatenatedDoc2Vec([models[0], models[1]])\n",
    "models_by_name['dbow+dmc'] = ConcatenatedDoc2Vec([models[0], models[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-10000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nfinished in 795.31973528862 seconds\nEvaluating Doc2Vec(dbow,d100,n5,mc2,t8)\nTraining data embeddings loaded.\nTest data embeddings inferred.\nTraining and testing logistic regression... Fitting 5 folds for each of 12 candidates, totalling 60 fits\n[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed:   12.4s\n[Parallel(n_jobs=4)]: Done  60 out of  60 | elapsed:   32.5s finished\nfinished in 38.262173652648926 seconds\nAccuracy: 0.88948 \n\nTraining Doc2Vec(\"alpha=0.05\",dm/m,d100,n5,w10,mc2,t8)... DOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nfinished in 963.8729212284088 seconds\nEvaluating Doc2Vec(\"alpha=0.05\",dm/m,d100,n5,w10,mc2,t8)\nTraining data embeddings loaded.\nTest data embeddings inferred.\nTraining and testing logistic regression... Fitting 5 folds for each of 12 candidates, totalling 60 fits\n[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed:    6.3s\n[Parallel(n_jobs=4)]: Done  60 out of  60 | elapsed:   14.0s finished\nfinished in 15.713765621185303 seconds\nAccuracy: 0.81804 \n\nTraining Doc2Vec(dm/c,d100,n5,w5,mc2,t8)... DOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nfinished in 1018.6434507369995 seconds\nEvaluating Doc2Vec(dm/c,d100,n5,w5,mc2,t8)\nTraining data embeddings loaded.\nTest data embeddings inferred.\nTraining and testing logistic regression... Fitting 5 folds for each of 12 candidates, totalling 60 fits\n[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed:    3.0s\n[Parallel(n_jobs=4)]: Done  60 out of  60 | elapsed:    6.1s finished\nfinished in 6.569892168045044 seconds\nAccuracy: 0.717 \n\nEvaluating Doc2Vec(dbow,d100,n5,mc2,t8)+Doc2Vec(\"alpha=0.05\",dm/m,d100,n5,w10,mc2,t8)\nTraining data embeddings loaded.\nTest data embeddings inferred.\nTraining and testing logistic regression... Fitting 5 folds for each of 12 candidates, totalling 60 fits\n[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed:   18.8s\n[Parallel(n_jobs=4)]: Done  60 out of  60 | elapsed:   54.7s finished\nfinished in 69.6707763671875 seconds\nAccuracy: 0.8884 \n\nEvaluating Doc2Vec(dbow,d100,n5,mc2,t8)+Doc2Vec(dm/c,d100,n5,w5,mc2,t8)\nTraining data embeddings loaded.\nTest data embeddings inferred.\nTraining and testing logistic regression... Fitting 5 folds for each of 12 candidates, totalling 60 fits\n[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed:   18.0s\n[Parallel(n_jobs=4)]: Done  60 out of  60 | elapsed:   57.0s finished\nfinished in 58.24980711936951 seconds\nAccuracy: 0.8876 \n\n"
    }
   ],
   "source": [
    "accuracies = {}\n",
    "\n",
    "for model_name, model in models_by_name.items():\n",
    "    if model_name.startswith(\"Doc2Vec\"):\n",
    "        print(f\"Training {model_name}... \", end='')\n",
    "        start = time()\n",
    "        model.train(td_reviews, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "        print(f\"finished in {time() - start} seconds\")\n",
    "        accuracies[str(model)] = train_lr_evaluate(model, data)\n",
    "        print(f\"Accuracy: {accuracies[str(model)]} \\n\")\n",
    "\n",
    "\n",
    "for model in [models_by_name['dbow+dmm'], models_by_name['dbow+dmc']]:\n",
    "    accuracies[str(model)] = train_lr_evaluate(model, data)\n",
    "    print(f\"Accuracy: {accuracies[str(model)]} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'Doc2Vec(\"alpha=0.05\",dm/m,d100,n5,w10,mc2,t8)': 0.81804,\n 'Doc2Vec(dbow,d100,n5,mc2,t8)': 0.88948,\n 'Doc2Vec(dbow,d100,n5,mc2,t8)+Doc2Vec(\"alpha=0.05\",dm/m,d100,n5,w10,mc2,t8)': 0.8884,\n 'Doc2Vec(dbow,d100,n5,mc2,t8)+Doc2Vec(dm/c,d100,n5,w5,mc2,t8)': 0.8876,\n 'Doc2Vec(dm/c,d100,n5,w5,mc2,t8)': 0.717}\n"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the results, DBOW embeddings alone performs as good as any other solution. Concatenating DBOW and DM vectors results at most in a negligible improvement, not justifying the extra time and computational power used to train and predict on an additional DM model.\n",
    "\n",
    "The accuracy of the paper (92.58%) was not achieved, as the best results we got were at most about 89% accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python37464bit0b4af82858af48809da7c8a8cbaf6f6a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}