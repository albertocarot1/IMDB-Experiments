{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec sentiment classification (IMDB)\n",
    "Doc2Vec is a sentence embeddings representation, first introduced by Le & Mikolov in the paper \"Distributed Representations of Sentences and Documents\" <http://cs.stanford.edu/~quocle/paragraph_vector.pdf>.\n",
    "In this notebook we will replicate part of the experiments described in the paper, more specifically the sentiment classification on the IMDB dataset.\n",
    "\n",
    "The experiment basically consists in two main models trained:\n",
    "\n",
    "1. Doc2Vec model trained on train+unlabeled data, to obtain document embeddings on IMDB dataset.\n",
    "2. Logistic Regression trained on train data's Doc2Vec embeddings.\n",
    "\n",
    "In order to test accuracy of the system, the Doc2Vec model is used first to infer an embedding for each test document. These embeddings are then used to predict the sentiment of the review through the trained Logistic Regression.\n",
    "\n",
    "First of all we download the IMDB dataset from tensorflow_datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "\n",
    "data = tfds.load(\n",
    "    name=\"imdb_reviews\",\n",
    "    as_supervised=False)\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create a class where the iterator is a generator that reads IMDB reviews, tokenizes them, and creates TaggedDocument to be fed into the Doc2Vec models. \n",
    "\n",
    "We need this class to stream data from disk during the Doc2Vec training and vocabulary creation. In this manner, the computer will not swap memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "from functools import lru_cache\n",
    "import re\n",
    "\n",
    "\n",
    "def tokenize_text(text: str) -> list:\n",
    "    \"\"\" Clean from html tags and tokenize \"\"\"\n",
    "    # Standardize newlines\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    temp = re.sub(cleanr, '\\n', text)\n",
    "\t\n",
    "    # Remove repetition of blank spaces, by keeping only one\n",
    "    temp = re.sub(\" +\", \" \", temp)\n",
    "\n",
    "    clean = temp.strip()\n",
    "    return gensim.utils.to_unicode(clean).split()\n",
    "\n",
    "\n",
    "class TxtCorpus(list):\n",
    "    \"\"\"\n",
    "    Iterable that returns TaggedDocument objects, used for Doc2Vec training.\n",
    "    Process documents one by one using generators, avoiding to fill up RAM.\n",
    "    \"\"\"\n",
    "    def __init__(self, tf_dataset, parts=['train', 'test', 'unsupervised']):\n",
    "        self.tf_dataset = tf_dataset\n",
    "        self.parts = parts\n",
    "\n",
    "    def __iter__(self):\n",
    "        for part in self.parts:\n",
    "            for i, line in enumerate(self.tf_dataset[part]):\n",
    "                text = line['text'].numpy().decode()\n",
    "                tokens = tokenize_text(text)\n",
    "                if i%10000 < 1:\n",
    "                    print(f\"DOCUMENT WITH ID(TAG): {part}-{i}\")\n",
    "                yield TaggedDocument(tokens, [f\"{part}-{i}\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embeddings for the training of the Logistic Regression model are calculated during the training of the Doc2Vec model. Instead, the embeddings for the test set, like in a real application scenario, must be inferred once the model is trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_from_model(d2v_model, tf_dataset, part):\n",
    "    \"\"\" Load embeddings created during training phase.\"\"\"\n",
    "    X = []\n",
    "    Y = []\n",
    "    for i, line in enumerate(tf_dataset[part]):\n",
    "        X.append(d2v_model.docvecs[f\"{part}-{i}\"])\n",
    "        Y.append(line['label'].numpy())\n",
    "    return np.asarray(X), np.asarray(Y)\n",
    "\n",
    "def infer_from_model(d2v_model, tf_dataset, part):\n",
    "    \"\"\" Infer embedding for a text not seen in training phase.\"\"\"\n",
    "    X = []\n",
    "    Y = []\n",
    "    for i, line in enumerate(tf_dataset[part]):\n",
    "        text = line['text'].numpy().decode()\n",
    "\n",
    "        X.append(d2v_model.infer_vector(tokenize_text(text)))\n",
    "        Y.append(line['label'].numpy())\n",
    "    return np.asarray(X), np.asarray(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training the logistic regression, a little bit of hyperparameter tuning is done in the process through grid search in a set of hyperparameters. The accuracy is then calculated on the best model found through a 5-fold cross validation. The limited number of features (typical of embeddings, compared to bag of words), allows us to do that relatively quickly. \n",
    "This way we expect to get that little extra boost in accuracy.\n",
    "\n",
    "Since the problem is binary classification, the dataset is balanced, and we do not have specific requirements on precision and recall, accuracy is an acceptable metric in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def train_lr_evaluate(model, data):\n",
    "    print(f\"Evaluating {model}\")\n",
    "    X_train, Y_train = load_from_model(model, data, 'train')\n",
    "    print(\"Training data embeddings loaded.\")\n",
    "\n",
    "    X_test, Y_test = infer_from_model(model, data, 'test')\n",
    "    print(\"Test data embeddings inferred.\")\n",
    "\n",
    "    lr = linear_model.LogisticRegression()\n",
    "\n",
    "    penalty = ['l1', 'l2']\n",
    "    C = [0.0001, 0.001, 0.1, 1, 10, 100]\n",
    "    hyperparameters = dict(C=C, penalty=penalty, solver=['liblinear'])\n",
    "\n",
    "    clf = GridSearchCV(lr, hyperparameters, cv=5, verbose=2, n_jobs=4)\n",
    "    print(f\"Training and testing logistic regression... \", end='')\n",
    "    start = time()\n",
    "    clf.fit(X_train, Y_train)\n",
    "    print(f\"finished in {time() - start} seconds\")\n",
    "    return clf.best_estimator_.score(X_test, Y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next steps, we combine unsupervised and train data for the Doc2Vec vocabulary creation and embeddings training. \n",
    "\n",
    "With some guidance from the gensim creators https://radimrehurek.com/gensim/auto_examples/howtos/run_doc2vec_imdb.html on parameters to optimize training time without affecting performances, we define the default parameters for the trained models:\n",
    "* ``vector_size=100``, as there not seems to be a decay in performances compared to the paper's size of 400.\n",
    "* ``epochs=20``, large enough for the model to learn features from the text.\n",
    "* ``min_count=2`` discards words that appear in only one document, and hence do not benefit from the training that relies on the co-occurence of words in different documents\n",
    "* ``sample=0``: no downsampling of frequent words\n",
    "* ``hs=0`` and ``negative=5``: 5 \"noise words\" are updated for every update of positive samples\n",
    " \n",
    "Starting from those base parameters, we train three models:\n",
    "\n",
    "* One with the Distributed Bag of word (``DBOW``) method described in the paper.\n",
    "* One with the Distiributed Memory (``DM``) method described in the paper, where the paragraph vector and word ``vectors are averaged during training`` to predict the next word in a context. \n",
    "* One with the Distiributed Memory (``DM``) method described in the paper, where the paragraph vector and word ``vectors are concatenated during training`` to predict the next word in a context. Concatenation results in a bigger and slower to train model, compared to averaging.\n",
    "\n",
    "In the paper, the authors noted that concatenating the trained DBOW and DM paragraph vectors improves performance. We will repeat the same experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "creating vocabularies...\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-5000\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-15000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-5000\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-15000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-25000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-35000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): unsupervised-45000\nDoc2Vec(dbow,d100,n5,mc2,t8) vocabulary created\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-5000\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-15000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-5000\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-15000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-25000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-35000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): unsupervised-45000\nDoc2Vec(\"alpha=0.05\",dm/m,d100,n5,w10,mc2,t8) vocabulary created\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-5000\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-15000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-5000\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-15000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-25000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-35000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): unsupervised-45000\nDoc2Vec(dm/c,d100,n5,w5,mc2,t8) vocabulary created\nvocabularies created in 249.76914882659912 seconds\n"
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from time import time\n",
    "\n",
    "import multiprocessing\n",
    "from collections import OrderedDict\n",
    "\n",
    "import gensim.models.doc2vec\n",
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "\n",
    "\n",
    "td_reviews = TxtCorpus(data, parts=['train', 'unsupervised'])\n",
    "\n",
    "common_params = dict(vector_size=100, epochs=20, min_count=2, sample=0, workers=multiprocessing.cpu_count(), negative=5, hs=0)\n",
    "\n",
    "models = [\n",
    "    # PV-DBOW training\n",
    "    Doc2Vec(dm=0, **common_params),\n",
    "    # PV-DM w/ default averaging; a higher starting learning rate may improve CBOW/PV-DM modes\n",
    "    Doc2Vec(dm=1, window=10, alpha=0.05, comment='alpha=0.05', **common_params),\n",
    "    # PV-DM w/ concatenation\n",
    "    Doc2Vec(dm=1, dm_concat=1, window=5, **common_params),\n",
    "]\n",
    "models_by_name = {}\n",
    "\n",
    "print(\"creating vocabularies...\")\n",
    "start = time()\n",
    "for model in models:\n",
    "    model.build_vocab(td_reviews)\n",
    "    print(f\"{model} vocabulary created\")\n",
    "    models_by_name[str(model)] = model\n",
    "print(f\"vocabularies created in {time() - start} seconds\")\n",
    "\n",
    "models_by_name['dbow+dmm'] = ConcatenatedDoc2Vec([models[0], models[1]])\n",
    "models_by_name['dbow+dmc'] = ConcatenatedDoc2Vec([models[0], models[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "pervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-25000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-35000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): unsupervised-45000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-5000\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-15000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-5000\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-15000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-25000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-35000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): unsupervised-45000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-5000\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-15000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-5000\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-15000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-25000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-35000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): unsupervised-45000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-5000\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-15000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-5000\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-15000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-25000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-35000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): unsupervised-45000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-5000\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-15000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-5000\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-15000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-25000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-35000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): unsupervised-45000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-5000\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-15000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-5000\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-15000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-25000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-35000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): unsupervised-45000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-5000\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-15000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-5000\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-15000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-25000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-35000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): unsupervised-45000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-5000\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-15000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-5000\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-15000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-25000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-35000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): unsupervised-45000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-5000\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-15000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-5000\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-15000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-25000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-35000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): unsupervised-45000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-5000\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-15000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-5000\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-15000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-25000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-35000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): unsupervised-45000\nfinished in 923.9168682098389 seconds\nEvaluating Doc2Vec(\"alpha=0.05\",dm/m,d100,n5,w10,mc2,t8)\nTraining data embeddings loaded.\nTest data embeddings inferred.\nTraining and testing logistic regression... Fitting 5 folds for each of 12 candidates, totalling 60 fits\n[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed:    6.5s\n[Parallel(n_jobs=4)]: Done  60 out of  60 | elapsed:   16.2s finished\nfinished in 18.460651874542236 seconds\n{'Doc2Vec(dbow,d100,n5,mc2,t8)': 0.88792, 'Doc2Vec(\"alpha=0.05\",dm/m,d100,n5,w10,mc2,t8)': 0.82004}\nTraining Doc2Vec(dm/c,d100,n5,w5,mc2,t8)... DOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-5000\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-15000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-5000\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-15000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-25000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-35000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): unsupervised-45000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-5000\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-15000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-5000\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-15000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-25000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-35000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): unsupervised-45000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-5000\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-15000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-5000\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-15000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-25000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-35000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): unsupervised-45000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-5000\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-15000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-5000\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-15000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-25000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-35000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): unsupervised-45000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-5000\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-15000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-5000\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-15000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-25000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-35000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): unsupervised-45000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-5000\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-15000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-5000\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-15000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-25000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-35000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): unsupervised-45000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-5000\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-15000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-5000\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-15000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-25000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-35000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): unsupervised-45000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-5000\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-15000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-5000\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-15000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-25000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-35000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): unsupervised-45000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-5000\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-15000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-5000\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-15000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-25000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-35000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): unsupervised-45000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-5000\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-15000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-5000\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-15000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-25000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-35000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): unsupervised-45000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-5000\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-15000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-5000\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-15000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-25000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-35000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): unsupervised-45000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-5000\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-15000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-5000\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-15000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-25000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-35000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): unsupervised-45000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-5000\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-15000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-5000\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-15000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-25000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-35000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): unsupervised-45000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-5000\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-15000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-5000\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-15000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-25000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-35000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): unsupervised-45000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-5000\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-15000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-5000\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-15000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-25000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-35000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): unsupervised-45000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-5000\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-15000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-5000\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-15000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-25000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-35000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): unsupervised-45000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-5000\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-15000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-5000\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-15000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-25000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-35000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): unsupervised-45000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-5000\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-15000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-5000\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-15000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-25000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-35000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): unsupervised-45000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-5000\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-15000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-5000\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-15000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-25000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-35000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): unsupervised-45000\nDOCUMENT WITH ID(TAG): train-0\nDOCUMENT WITH ID(TAG): train-5000\nDOCUMENT WITH ID(TAG): train-10000\nDOCUMENT WITH ID(TAG): train-15000\nDOCUMENT WITH ID(TAG): train-20000\nDOCUMENT WITH ID(TAG): unsupervised-0\nDOCUMENT WITH ID(TAG): unsupervised-5000\nDOCUMENT WITH ID(TAG): unsupervised-10000\nDOCUMENT WITH ID(TAG): unsupervised-15000\nDOCUMENT WITH ID(TAG): unsupervised-20000\nDOCUMENT WITH ID(TAG): unsupervised-25000\nDOCUMENT WITH ID(TAG): unsupervised-30000\nDOCUMENT WITH ID(TAG): unsupervised-35000\nDOCUMENT WITH ID(TAG): unsupervised-40000\nDOCUMENT WITH ID(TAG): unsupervised-45000\nfinished in 991.7459635734558 seconds\nEvaluating Doc2Vec(dm/c,d100,n5,w5,mc2,t8)\nTraining data embeddings loaded.\nTest data embeddings inferred.\nTraining and testing logistic regression... Fitting 5 folds for each of 12 candidates, totalling 60 fits\n[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed:    3.1s\n[Parallel(n_jobs=4)]: Done  60 out of  60 | elapsed:    6.4s finished\nfinished in 6.806178331375122 seconds\n{'Doc2Vec(dbow,d100,n5,mc2,t8)': 0.88792, 'Doc2Vec(\"alpha=0.05\",dm/m,d100,n5,w10,mc2,t8)': 0.82004, 'Doc2Vec(dm/c,d100,n5,w5,mc2,t8)': 0.71744}\nEvaluating Doc2Vec(dbow,d100,n5,mc2,t8)+Doc2Vec(\"alpha=0.05\",dm/m,d100,n5,w10,mc2,t8)\nTraining data embeddings loaded.\nTest data embeddings inferred.\nTraining and testing logistic regression... Fitting 5 folds for each of 12 candidates, totalling 60 fits\n[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed:   20.7s\n[Parallel(n_jobs=4)]: Done  60 out of  60 | elapsed:   57.2s finished\nfinished in 58.94800806045532 seconds\n{'Doc2Vec(dbow,d100,n5,mc2,t8)': 0.88792, 'Doc2Vec(\"alpha=0.05\",dm/m,d100,n5,w10,mc2,t8)': 0.82004, 'Doc2Vec(dm/c,d100,n5,w5,mc2,t8)': 0.71744, 'Doc2Vec(dbow,d100,n5,mc2,t8)+Doc2Vec(\"alpha=0.05\",dm/m,d100,n5,w10,mc2,t8)': 0.88776}\nEvaluating Doc2Vec(dbow,d100,n5,mc2,t8)+Doc2Vec(dm/c,d100,n5,w5,mc2,t8)\nTraining data embeddings loaded.\nTest data embeddings inferred.\nTraining and testing logistic regression... Fitting 5 folds for each of 12 candidates, totalling 60 fits\n[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed:   16.5s\n[Parallel(n_jobs=4)]: Done  60 out of  60 | elapsed:   53.7s finished\nfinished in 54.93923854827881 seconds\n{'Doc2Vec(dbow,d100,n5,mc2,t8)': 0.88792, 'Doc2Vec(\"alpha=0.05\",dm/m,d100,n5,w10,mc2,t8)': 0.82004, 'Doc2Vec(dm/c,d100,n5,w5,mc2,t8)': 0.71744, 'Doc2Vec(dbow,d100,n5,mc2,t8)+Doc2Vec(\"alpha=0.05\",dm/m,d100,n5,w10,mc2,t8)': 0.88776, 'Doc2Vec(dbow,d100,n5,mc2,t8)+Doc2Vec(dm/c,d100,n5,w5,mc2,t8)': 0.88796}\n"
    }
   ],
   "source": [
    "accuracies = {}\n",
    "\n",
    "for model_name, model in models_by_name.items():\n",
    "    if model_name.startswith(\"Doc2Vec\"):\n",
    "        print(f\"Training {model_name}... \", end='')\n",
    "        start = time()\n",
    "        model.train(td_reviews, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "        print(f\"finished in {time() - start} seconds\")\n",
    "        accuracies[str(model)] = train_lr_evaluate(model, data)\n",
    "        print(f\"Accuracy: {accuracies[str(model)]} \\n\")\n",
    "\n",
    "\n",
    "for model in [models_by_name['dbow+dmm'], models_by_name['dbow+dmc']]:\n",
    "    accuracies[str(model)] = train_lr_evaluate(model, data)\n",
    "    print(f\"Accuracy: {accuracies[str(model)]} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'Doc2Vec(\"alpha=0.05\",dm/m,d100,n5,w10,mc2,t8)': 0.82004,\n 'Doc2Vec(dbow,d100,n5,mc2,t8)': 0.88792,\n 'Doc2Vec(dbow,d100,n5,mc2,t8)+Doc2Vec(\"alpha=0.05\",dm/m,d100,n5,w10,mc2,t8)': 0.88776,\n 'Doc2Vec(dbow,d100,n5,mc2,t8)+Doc2Vec(dm/c,d100,n5,w5,mc2,t8)': 0.88796,\n 'Doc2Vec(dm/c,d100,n5,w5,mc2,t8)': 0.71744}\n"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the results, DBOW embeddings alone performs as good as any other solution. Concatenating DBOW and DM vectors results at most in a negligible improvement, not justifying the extra time and computational power used to train and predict on an additional DM model.\n",
    "\n",
    "The accuracy of the paper (92.58%) was not achieved, as the best results we got were at most about 89% accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python37464bit0b4af82858af48809da7c8a8cbaf6f6a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}